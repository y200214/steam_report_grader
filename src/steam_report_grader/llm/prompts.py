# src/steam_report_grader/llm/prompts.py
from __future__ import annotations
from textwrap import dedent

def build_scoring_prompt(
    question_label: str,
    question_text: str,
    rubric_text: str,
    answer_text: str,
    max_score: int = 5,
) -> str:
    """
    絶対評価用プロンプト。
    出力は JSON 固定にして後処理しやすくする。
    """
    prompt = f"""
          You are an expert in STEAM education and in educational assessment.
          Based on the following question and rubric, rigorously evaluate the student's answer.

          [Question ID]
          {question_label}

          [Question]
          {question_text.strip()}

          [Rubric]
          {rubric_text.strip()}

          [Student Answer]
          {answer_text.strip()}

Instructions:
1. Based on the rubric, score the answer from 0 to {max_score}.
2. Assign subscores for each evaluation criterion.
3. Create the following two types of explanations:
   - summary_bullets: Up to 3–4 bullet points that briefly explain why this score was given.
   - detailed_explanation: A longer explanation of about 200–400 Japanese characters that carefully explains the reasons for the evaluation.
4. In evidence, include short quotes from the student's answer that support your evaluation.
   - Passages you consider important in this answer and that influenced your evaluation.
   - About 20–30 Japanese characters per quote.
   - In aspect, write which evaluation criterion the quote corresponds to.
5. Your output must consist only of the following JSON structure and nothing else.
6. The output must be in the "form of a Python dict literal" and include the keys below.
   - Key names may use either double quotes or single quotes.
   - Do not use double quotes (") inside string values; use single quotes (') instead.

Output format:
        {{
          "score": 数値,
          "subscores": {{
            "観点1": 数値,
            "観点2": 数値
          }},
          "summary_bullets": [
            "箇条書き1",
            "箇条書き2"
          ],
          "detailed_explanation": "長文の説明",
          "evidence": [
            {{
              "aspect": "観点名",
              "quote": "受験者の回答からの短い引用"
            }}
          ]
        }}
        """
    return dedent(prompt).strip()

def build_final_evaluation_prompt(
    student_id: str,
    question: str,
    ai_sim_score: float,
    peer_sim_score: float,
    symbolic_score: float,
    answer_text: str,
) -> str:
    """
    最終的な AI疑惑スコアを算出するためのプロンプト
    """
    prompt = f"""
You are an expert in educational assessment and AI template detection.
Below are several features related to a student's response.
Based on this information, determine how likely it is that the response was written by AI, using a score between 0 and 1, and explain the reasons.
Your output must be *only* in the following JSON format:
[Student ID]

    {student_id}

    [Question]
    {question}

    [Answer]
    {answer_text.strip()}

[Features]

-Similarity to AI reference answer (ai_similarity_score): {ai_sim_score:.2f}
-Similarity to other students’ responses (peer_similarity_score): {peer_sim_score:.2f}
-Symbolic feature score (symbolic_score): {symbolic_score:.2f}

1. Evaluate the above features comprehensively and assign a score between 0.0 (not AI-like at all) and 1.0 (highly AI-like) for how likely this student’s response is to be a “template-like response generated by AI.”
2. Explain the reasons for this score from the following perspectives:
-Structure (the introduction, body, and conclusion are too neatly organized)
-Formulaic expressions (phrases frequently used by AI, positive bias)
-Uniformity of writing style (sentence length, vocabulary usage, etc.)
-Overly typical examples (content similar to that of other students)
Your output must be "only" in the following JSON format.
Output:
    {{
      "ai_likeness_score": number,
      "ai_likeness_comment": "Reason for the evaluation"
    }}
    """
    return dedent(prompt).strip()